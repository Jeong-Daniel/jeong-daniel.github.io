---
title: 	ResNet의 이해와 정리
date:   2022-09-20 15:07:41 +0900
categories: [Computer_Vision, Study]
tags: [review, vision]
---

[Deep Residual Learning for Image Recognition](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)


ResNet, 'Deep Residual Learning for Image Recognition' 을 정리할려고 합니다. 논문 원문은 위 링크를 참고하시면 됩니다.

ResNet 모델 소개
CNN에서는 크게 ‘거대 구조’의 대표적 사례라고 할 수 있는 인셉션 모델과 ‘깊은 구조’의 대표적 사례라고 할 수 있는 레스넷 모델이 있습니다. ResNet은 residual repesentation 함수를 학습함으로써 신경망이 152 layer까지 가질 수 있습니다.

![img1 daumcdn](https://user-images.githubusercontent.com/85277660/211158115-020256b9-e2d7-4062-b758-29467d0cb420.jpg)

ResNet은 이전 layer의 입력을 다음 layer로 전달하기 위해 skip connection(또는 shorcut connection)을 사용합니다. 이 skip connection은 깊은 신경망이 가능하게 하고 ResNet은 ILSVRC 2015 우승을 했습니다.

---

### 기존 Plain Network에서 레이어를 늘릴때의 문제점

![score graph](https://user-images.githubusercontent.com/85277660/211158120-3a7d85f1-8590-4671-bf90-7c66745d43aa.png)

Plain network는 Plain network는 skip connection을 사용하지 않은 일반적인 CNN(AlexNet, VGGNet) 신경망을 의미합니다. 해당 네트워크는 점점 깊어질 수록 기울기(gradient) 소실(vanishing)과 폭발(exploding) 문제가 발생합니다. 각 그래프 노드에 따라서 기울기를 구하기 위해서 손실함수의 미분을 오차역전파를 사용하는데 이 과정에서 활성화 함수의 편미분을 구하고 그 값을 곱해줍니다. 즉 레이어가 뒤로 전파되는 과정에서 활성화 함수의 미분값이 점점 작아지거나 커지는데 이 과정이 깊어지면 기울기의 크기가 0에 도달해서 소실되거나 아주 커져서 폭발적으로 늘어날 가능 성이 있습니다.

---

### Skip Connection

그래서 위의 기울기 소실/폭발 문제를 해결하기 위해서 입력 x를 몇 layer이후의 출력값에 더해주는 skip connection을 더해줍니다.

![bottle neck](https://user-images.githubusercontent.com/85277660/211158142-48f0b05a-614c-42fe-af8f-9d890e6d0245.png)

* x : 입력값
* F(x) : CNN Layer -> ReLU -> CNN Layer 을 통과한 출력값
* H(x) : CNN Layer -> ReLU -> CNN Layer -> ReLU 를 통과한 출력값

기존 신경망은 H(x)=x가 되도록 학습했습니다. 여기서는 skip connection에 의해 출력값에 x를 더하고 H(x) = F(x) = x로 정의합니다. 그리고 F(x) = 0이 되도록 학습하여 H(x) = 0 + x가 되게 합니다. 이러면 미분을 하더라도 x가 1이 되어 기울기 소실 문제가 해결됩니다.

log문제를 처리할때나 1과 같은 작은 수를 더해서 소실되지 않도록 하는 테크니는 생각보다 자주 보이는거 같네요 아무튼 이 방법으로 기울기 소실을 억제하여 정확도가 감소하지 않고 레이어를 깊게 쌓을수 있어 더 나은 신경망을 구축할 수 있게 되었습니다.

![Skip Connection](https://user-images.githubusercontent.com/85277660/211158163-9205edc5-e62a-4af9-8136-ce19ac02df70.png)

34-layer ResNet with Skip Connection (Top), 34-layer Plain Network (Middle), 19-layer VGG-19 (Bottom)

아래서부터 VGG19에서 이를 34개로 늘린 네트워크 그리고 skip connection을 추가한 ResNet을 볼 수 있습니다.

![Skip Connection](https://user-images.githubusercontent.com/85277660/211158168-0a093601-6c5d-42d1-b4bb-5107b8efd4b6.png)

Plain neural network와 비교했을 때 Layer를 상당히 깊게 쌓았음에도 적게 쌓았을 때보다 에러가 낮으며 degradation problem이 발생하지 않은 것을 알 수 있습니다.

![Skip Connection](https://user-images.githubusercontent.com/85277660/211158175-b9908e2b-c06b-4e40-a235-b646fb536dbc.png)

### Bottleneck Design

skip connection으로 레이어를 깊게 쌓을 수 있는 것 까지는 좋았으나 레이어가 늘어난다는 것은 연산량 역시 굉장히 증가를 하게 될 것입니다. 그래서 Bottlenect design은 다음과 같이 신경망 복잡도를 감소하기위해 사용됩니다.

![Bottleneck Design](https://user-images.githubusercontent.com/85277660/211158185-f614db70-8775-456b-ab1a-539cf9160ae0.png)

기존의 Residual Block은 한 블록에 Convolution Layer(3X3) 2개가 있는 구조였습니다. Bottleneck 구조는 오른쪽 그림의 구조로 바꾸었는데 층이 하나 더 생겼지만 Convolution Layer(1X1) 2개를 사용하기 때문에 파라미터 수가 감소하여 연산량이 줄어들었습니다. 또한 Layer가 많아짐에 따라 Activation Function이 증가하여 더 많은 non-linearity가 들어갔습니다. 즉 Input을 기존보다 다양하게 가공할 수 있게 되었습니다. 이 기법은 NIN과 GoogLeNet에서 제안되었습니다. 1x1 conv는 신경망의 성능을 감소시키지 않고 파라미터 수를 감소시킵니다.

이렇게 ResNet은 Skip Connection을 이용한 Shortcut과 Bottleneck 구조를 이용하여 더 깊게 층을 쌓을 수 있었습니다.

---

### 결론

![conclusion](https://user-images.githubusercontent.com/85277660/211158199-9a595d43-7f02-45d4-a1dd-45248ce4e3d9.png)

ResNet 모델을 이용하여 보다 깊은 레이어를 보다 좋은 성능을 이끌어내는 것에는 성공하였지만. 결국 논문에서는 이론을 바탕으로 결과를 도출한 것이 아닌 경험적/실험의 결과입니다. 그래서 여전히 왜 깊은 레이어는 어떻게 도움을 주는지에 대해서 많은 설명을 시도하고 있습니다.