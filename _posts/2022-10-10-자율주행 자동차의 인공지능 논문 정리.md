---
title: 	자율주행 자동차의 인공지능 논문 정리
date:   2022-10-10 16:14:41 +0900
categories: [Computer_Vision, Study]
tags: [review, vision, ai, motor, automatic]
---

[https://www.dbpia.co.kr/pdf/pdfView.do?nodeId=NODE07119731 ](https://www.dbpia.co.kr/pdf/pdfView.do?nodeId=NODE07119731 )

인공지능의 발전은 놀라운 수준으로 올라왔고 그중 테슬라의 자율주행은 항상 컴퓨터 비전을 언급하면 자연스럽게 따라오는 주제입니다. 실제로 같은 거리를 이동할때 사람보다 테슬라의 자율주행자동차의 사고율이 현저하게 낮게 나옵니다. 다만 테슬라의 자율주행이 사고가 나면 언론에 노출되는 확률이 높으니 빈도에서 오는 착각을 불러 일으킬 때도 있습니다.

![car rate](https://user-images.githubusercontent.com/85277660/211181027-d961c7c1-8554-4b56-acd4-c39e91052647.png "테슬라와 일반차량 사고율 비교 2021년 2분기 [자료: 테슬라/NHTSA]")

*테슬라의 오토파일럿 기능을 활성화했을 때 사고율은 441만마일(약 710만km)마다 1건에 불과했다. 반면, 오토파일럿 기능을 사용하지 않은 테슬라 차량의 평균 사고율은 120만마일(약 193만km)마다 1건


지금의 자율주행 기술은 단순히 차량에 머무르지 않고 스마트팩토리의 로봇이동, 자율주행 선박, 드론 등 광범위하게 퍼져나가고 있습니다. 어찌보면 고리타분하고 당연하게 느껴지는 자율주행에 대해서 다시 한번 정리를 해볼려고 합니다.

![img](https://user-images.githubusercontent.com/85277660/211181040-766292b0-449f-44ea-bac4-ab0a0b8fb62b.png)

자율주행 자동차는 인지-판단-제어 3단계를 거치면서 작동을 하게 됩니다.

카메라와 라이더, 레이더등 센서를 통해서 주변 환경을 인지하고 인지된 환경을 바탕으로 주행 경로를 생성하고 마지막으로 그 경로를 바탕으로 경로를 추종하도록 제어가 이루어 집니다.

위 사진의 예시에서는 이미지 세그멘테이션으로 객체 단위로 구분하고 이를 분류한 정보를 바탕으로 주행을 합니다. 이때 우리는 시각정보만으로 도로환경을 인지하고 자동차를 몰 수 있지만 컴퓨터에게는 사뭇 다른 이야기입니다.

2D-3D의 공간소실에 대해서 간단하게 설명을 드리자면 카메라로 인식하는 세계는 2D입니다. 데이터의 단위는 3차원이지만 이는 가로x세로x색상공간이기 때문에 사실상 가로세로의 픽셀조합으로 이루어집니다. 이는 사람도 마찬가지입니다.미국의학협회의 기준표에 의하면 한쪽 눈의 실명으로 인한 노동능력상실률은 24%(양안 실명의 경우 85%)인데 이는 공간이라는 개념은 학습의 대상이기 때문입니다.

예로 선천적인 시각장애인이 나중에 기술이나 이식을 통해 시각을 되찾게 되더라도 계단을 보게되면 벽에다가 줄그어놓은 것으로 보인다고 합니다. 우리는 오랜기간 학습을 통해서 각 계단의 줄의 거리는 서로 다른 것을 알지만 이 과정이 없다면 계단은 다르게 보이고 컴퓨터비전의 단안카메라 역시 마찬가지 입니다. 3D공간을 2D로 옮겨오게 되면서 정보의 소실이 일어 났습니다. 이를 극복하기 위해서 양안카메라를 사용하고 같은 객체에 대해 두 카메라의 상에 맺는 픽셀차를 통해서 거리를 구하고 공간을 형성할 수 있습니다.(또는 단안카메라를 사용하더라도 우리가 추적하고자 하는 객체의 크기를 미리 알고 있다면 어느정도 거리를 구할 수는 있습니다)

하지만 자율주행자동차는 지금 당장의 운전보다도 좀더 넓은 공간과 방향을 학습하고 추론하고자 합니다. 이제 카메라로는 기능을 수행하기 어렵습니다. 레이더와 라이다. 다양한 비전 센서를 통해서 공간을 구성해봅시다. 그러면 자율주행자동차는 공간을 어떻게 인식하는지에 대해서 봅시다.

### SLAM (Simultaneous localization and mapping)

![img SLAM](https://user-images.githubusercontent.com/85277660/211181054-e48214f4-430a-41ee-9fe2-b60b580d7158.png)

공간을 인식하고 이를 바탕으로 행동하는 것은 자율주행에만 국한되는 것이 아닙니다. 작은 로봇청소기 마저도 공간을 구성하고 추론하는 기능이 없다면 무작위적으로 청소를 수행하게 될 것이고 이는 분명히 반복적인 곳을 청소하거나 어떤 곳은 청소를 전혀 하지 않는등 비효율적인 지점이 생기게 됩니다.

[인텔 AI 칩·라이다·3D 센서 탑재…자율車 기술 장착한 로봇청소기](https://www.hankyung.com/economy/article/2021011111951)

선반을 정렬하는 로봇, 차량의 자동주차, 드론 조종, 택배 배송 다양한 분야의 사용을 위해서 SLAM(동시적 위치추정 및 지도작성)은 유용하게 사용할 수있습니다.

![sensor](https://user-images.githubusercontent.com/85277660/211181065-6845507f-7303-4fa6-98fc-c4d997e78867.png)

SLAM의 처리 흐름은 두단계로 나누어 볼 수 있습니다. 센서를 통해서 프론트단의 신호처리 그리고 이 신호를 바탕으로 그래프 최적화 처리를 수행합니다.

여기서 프론트의 구성요소를 좀더 디테일하게 살펴보게 시각적인 SLAM와 라이더 SLAM을 살펴봅니다.

시각적인 SLAM

앞에서 2D-3D 공간 소실에 대한 이야기를 설명했는데 시각적인 SLAM의 장단점역시 그대로 오게 됩니다. 비교적 저렴한 카메라를 사용해서 저비용으로 구현할 수 있다는 장점이 있습니다. 하지만 단일 카메라는 심도(거리) 추정이 어려우며 식별마커, 체커보드 같은 기준이 되는 객체와 같이 사용합니다.


### 라이다(Light Detection and Ranging) SLAM

단안카메라는 심도추정이 어려우나 레이저 센서를 통해서 거리를 추정할 수 있는 장점이 있습니다. (물론 가격은 좀 나갑니다) 레이저 센서에서 얻어지는 출력값은 2차원 또는 3차원 포인트 클라우드 데이터로 얻어지게 되는데 고정밀 거리 측정이 가능하여 지도 생성에 효과적인 장점이 있습니다. 

![img1 daumcdn](https://user-images.githubusercontent.com/85277660/211181073-6e35997d-759c-4d5b-b26a-bad5e32c4242.png)

포인트 클라우드란 3D 레이저 스캐너(예: Trimble 3D 레이저 스캐너)로 생성한 객체의 표면에서 측정된 점의 그룹입니다.


하지만 포인트 클라우드는 밀도높게 생성되지 않습니다. 장애물이 많이 없는 곳에서는 포인트 클라우드를 정렬하기 어렵고, 높은 수준의 처리 능력이 필요합니다. 그래서 바퀴 기준 주행거리 측정, GNSS(범지구 위성 항법 시스탬), IMU(Inertial Measurement Unit) 관성추정 장치의 데이터와 결과를 융합하는 작업이 필요하기도 합니다.

![img1 daumcdn](https://user-images.githubusercontent.com/85277660/211181075-becb6e90-7b68-4832-912d-3fda92e258f0.jpg "3차원 라이다를 사용한 SLAM")

### SLAM의 문제

1. 실제 값과의 상당한 편차를 초래하는 위치추정 오차 누적

SLAM은 현재 위치를 시작해서 순차적으로 공간을 추정하는데 이 과정에서 약간의 오차가 발생하고 더 넓은 공간, 시간이 지날 수록 오차가 누적이 됩니다. 그러면 최종 결과는 지도 데이터의 붕괴나 왜곡이 일어날 만큼 오차가 발생할 수 있습니다. 이를 극복하기 위해서 여러 특징점을 기억하여 추정의 오차를 최소화 하는 것입니다. 즉 체크포인트를 만들어 나가는 방식으로 조정을 시도합니다.

 

2. 위치추정 실패 및 지도상 위치 상실

포인트 클라우드 지도 작성은 인식하는 대상의 움직임 특성을 고려하지 않습니다. 자동차의 경우 굉장히 다양한 속도를 가지게 되는데 이를 같은 속도로 인식하고 지도를 그리게 될경우 엉망이 됩니다. 그래서 위치추정 실패가 발생할 수 있는데 다른 복원 알고리즘이나 센서 데이터와 같이 사용합니다.

 

모션 모델에 센서 융합을 사용방법은 칼만필터와 입자필터(몬테카를로 위치추정)을 주로 사용하며 센서는 AHRS(자세방위기준장치), INS(관성항법시스탬) 가속도계센서, 자이로센서, 자기센서, IMU같은 관성측정기, 바퀴의 휠인코더

 

3. 영상 처리, 포인트 클라우드 처리 및 최적화에 소요되는 높은 계산 비용


### 레이더(RADAR)와 라이더(LiDAR)차이

라이다는 레이저를 매개로, 레이더는 전파를 매개로 사물을 파악합니다.

![img](https://user-images.githubusercontent.com/85277660/211181091-3a421c37-46ff-44d8-a272-beb1ed32c5f1.gif)

레이더는 전파이용 탐지 및 거리 측정(RAdio Detection And Ranging)의 줄임말로, 조사한 전자파가 대상에 부딪힌 뒤 되돌아오는 반사파를 측정하여 대상을 탐지하고 그 방향, 거리, 속도 등을 파악하는 정보 시스템을 이릅니다.

 
### Light Detection And Ranging, LiDAR

빛 탐지 및 범위측정(Light Detection And Ranging)의 약자. 이름에서 보듯이 레이더(RADAR)에서 전파가 레이저로 바뀌었다고 보면 되며, 아예 레이저 레이다라고도 부르기도 합니다. 초당 수백 만개에 달하는 레이저 펄스를 발사하여되돌아오는 시간을 측정하여 스캔하는 기술로 흔히 보는 레이저 거리측량기 같은 장비가 이에 해당됩니다.

![img1 daumcdn](https://user-images.githubusercontent.com/85277660/211181100-a673e155-b6ab-463e-b83f-ee59cf5d82ec.png)

라이더는 많은 장점이 있습니다. 레이더는 측정이 불가능한 온도, 물질분포등을 감지할 수 있으며, 야간에도 초음파나 카메라보다 안정적으로 주변환경을 스캐닝 할 수 있습니다.

![img1 daumcdn](https://user-images.githubusercontent.com/85277660/211181103-d11578ad-8cbc-4cf6-bc7e-4dd32760f7cb.png)

다만 단점도 많은데 높은 비용, 탐색범위의 제한, 높은 전력소모, 센서 앞에 장애물이 있으면 안됩니다. 그래서 위 현대 아이오닉 5 로보택시처럼 드러내고 사용해야 합니다. 

 

이력서 써야하는데... 이거 쓴다고 시간을 부었군요

앞으로? 공부할거 칼만필터, 입자필터(몬테카를로 위치추정)....